# CNN, Activation Function, Batch Normalization, Transfer Learning, Knowledge Distillation

## 1. CNN (합성곱 신경망)
CNN(Convolutional Neural Network)은 이미지와 같은 격자 구조 데이터를 처리하도록 설계된 신경망으로, 공간적 계층 구조를 캡처하는 데 중점을 둔다.

- **Convolution Layer (합성곱 층)**: 입력 이미지에 다양한 필터를 적용하여 특징을 추출한다. 이를 통해 모델이 이미지 내의 패턴을 인식할 수 있다.
- **비선형성 도입**: CNN에서는 선형 변환만으로는 복잡한 함수 표현이 어렵기 때문에 활성화 함수를 통해 비선형성을 추가하여 더 복잡한 패턴 학습이 가능하다.

## 2. 활성화 함수 (Activation Function)
활성화 함수는 신경망의 각 층에서 비선형성을 추가하는 역할을 한다.
- **역할**: 단순히 여러 층을 쌓는 것만으로는 모델이 선형 변환에 머물 수밖에 없으므로, 활성화 함수를 통해 모델이 더 복잡한 특성을 학습할 수 있도록 한다.

## 3. 배치 정규화 (Batch Normalization)
배치 정규화는 학습 속도를 높이고 과적합을 줄이기 위해 미니 배치 내의 데이터 분포를 정규화하는 기법이다.
- **미니 배치**: 전체 데이터셋이 아닌 일부 데이터 배치를 사용하여 학습하며, 메모리 효율을 높이고 병렬 처리를 가능하게 한다.
- **내부 공변량 변화**: 배치 단위로 학습할 때 계층별로 입력 데이터의 분포가 달라지는 현상을 줄이기 위해 적용한다.

## 4. 전이 학습 (Transfer Learning)
전이 학습은 기존에 학습된 모델의 지식을 새로운 작업에 활용하는 기법으로, 소규모 데이터셋으로도 효과적으로 학습할 수 있게 한다.
- **적용 예**: ImageNet과 같은 대규모 데이터셋에서 학습한 모델의 엣지, 텍스처, 색상 분포 등의 정보를 활용하여 다른 유사한 작업에 적용할 수 있다.
- **LoRA**: 큰 모델을 효율적으로 파인튜닝하기 위한 방법으로, 차원이 낮은 정보만을 활용하여 학습 자원을 절약한다.

## 5. 지식 증류 (Knowledge Distillation)
지식 증류는 큰 모델(Teacher)의 지식을 작은 모델(Student)에게 전달하는 학습 방법이다.
- **Teacher-Student 학습**: Teacher 모델이 예측한 소프트 타겟(예: 강아지일 확률 60%, 고양이일 확률 30%)을 Student 모델이 학습하여 더 작은 모델에서도 Teacher 모델의 성능을 유사하게 구현할 수 있다.
- **불확실성 정보 학습**: 단순히 클래스만 예측하는 것이 아니라, 각 클래스 간의 관계까지 학습하여 더 세밀한 판단을 가능하게 한다.
